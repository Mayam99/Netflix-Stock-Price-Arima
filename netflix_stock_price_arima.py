# -*- coding: utf-8 -*-
"""Netflix Stock Price-Arima.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l0_DWYpGpKcG3W1a-WBxO8eOuX6gAwqV

##Introduction
###This project focuses on time series analysis using the Netflix Stock Price dataset available on Kaggle. The dataset includes historical stock price data for Netflix (NFLX), providing crucial financial metrics such as opening, closing, highest, and lowest prices, along with the stock volume over a specific time period. The objective of this analysis is to forecast future stock prices using the ARIMA (AutoRegressive Integrated Moving Average) model, a powerful tool for time series forecasting.

###Time series forecasting for stock prices is crucial for financial decision-making, especially in trading and investments. By leveraging the ARIMA model, we aim to capture the historical trends and patterns in the Netflix stock data to make accurate future predictions. This analysis will cover data preprocessing, exploratory data analysis, model building, and performance evaluation.

##Importing Necessary Libraries.
"""

import pandas as pd #It imports the pandas library and assigns it the alias "pd" for easier use.
import numpy as np #It brings in a tool called numpy, nicknamed "np", to help with number crunching in your code.
import matplotlib.pyplot as plt #It imports plotting tools from matplotlib, nicknamed "plt", for creating visualizations.
import seaborn as sn #It imports the seaborn library, nicknamed "sns", for making statistical visualizations

"""##Loading the Dataframe."""

df = pd.read_csv("NFLX.csv") #It reads data from the "NFLX.csv" file and stores it in a table called "df" using pandas

df.head()

"""###ARIMA requires the dataset to be in a time-series format, meaning your index should be a datetime object."""

df.columns #It shows the names of all the columns in the DataFrame df.

df.dtypes #It shows the data type of each column in the DataFrame df.

df["Date"]= pd.to_datetime(df["Date"])
#This line converts the 'Month' column in your DataFrame (df) to datetime objects.
#This is important for time series analysis as it allows pandas to understand the temporal order of your data.
df #Shows the updated DataFrame.

df.dtypes #It shows the data type of each column in the DataFrame df.

# Set the Date column as the index
df.set_index('Date', inplace=True)
df

from matplotlib import pyplot as plt
df['Close'].plot(kind='hist', bins=20, title='Close')
plt.gca().spines[['top', 'right',]].set_visible(False)
#It creates a histogram of the '#Passengers' data, with 20 bins

# Check for missing data
df.isnull().sum()

import matplotlib.pyplot as plt

# Plotting the closing price over time
plt.figure(figsize=(10,6))
plt.plot(df['Close'], label='Netflix Closing Price')
plt.title('Netflix Stock Price Over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

data = df['Close']

from statsmodels.tsa.stattools import adfuller
adf,pvalue,usedlag_, nobs_, critical_values, icbest_ = adfuller(data)

"""###It performs the Augmented Dickey-Fuller test (ADF) to check if the time series data in df is stationary and stores the results in several variables. The test helps determine if the data has a unit root, which would indicate non-stationarity."""

print(pvalue) #if pvalue > 0.05 then data is not stationary

df["year"]= [d.year for d in df.index] #It extracts the year from the 'Date' index and creates a new 'year' column in the DataFrame.

df

df['month'] = df.index.strftime('%b') #It extracts the month abbreviation from the 'Date' index and creates a new 'month' column.
df # df then displays the updated DataFrame.

years= df['year'].unique() #It gets all unique years from the 'year' column and stores them in the years variable.
years

months= df['month'].unique() #It retrieves all the unique month abbreviations from the 'month' column and stores them in the months variable.
months

df['Day'] = df.index.day #It extracts the day of the month from the 'Date' index and creates a new 'Day' column.

df['Day']

dates= df['Day'].unique() #It gets all unique days of the month from the 'Day' column and stores them in dates.
dates

sn.boxplot(x='year',y='Close',data=df) #It creates a box plot to visualize the distribution of 'Close' prices (y-axis) for each year (x-axis) using the seaborn library.

sn.boxplot(x='month', y='Close', data=df) #It creates a box plot showing the distribution of 'Close' prices for each month.

df_close = df[['Close']] #It creates a new DataFrame called df_close containing only the 'Close' column from the original DataFrame df.
df_close

# Ensuring the index has a frequency
df.index = pd.to_datetime(df.index)  # Making sure the index is datetime

# Setting the frequency to 'B' for business days.
df.index.freq = pd.infer_freq(df.index) #It attempts to automatically determine the frequency of the DataFrame's index

# Keep only the 'Close' column and maintain the index
df_close = df[['Close']]
df_close

# Resample the DataFrame to ensure a complete index (daily frequency)
df_resampled = df_close.resample('B').ffill()  # Forward filling to handle missing values

# It sets the frequency of the df_resampled DataFrame's index to 'B', representing business days.
df_resampled.index.freq = 'B'

from statsmodels.tsa.seasonal import seasonal_decompose
# Performing seasonal decomposition
decompose = seasonal_decompose(df_resampled['Close'], model='additive')

trend= decompose.trend #This line extracts the overall upward or downward movement (trend) from the decomposed time series data and assigns it to the variable trend.
seasonal=decompose.seasonal #This line extracts the repeating patterns or cycles (seasonality) within the data and assigns it to the variable seasonal.
residual=decompose.resid #This line extracts the remaining noise or random fluctuations (residuals) after removing the trend and seasonality and assigns it to the variable residual.

trend

seasonal

residual

plt.figure(figsize=(12,8))
plt.subplot(411)
plt.plot(df_resampled["Close"],label="Orginal",color='red')
plt.legend(loc='upper left')
plt.subplot(412)
plt.plot(trend,label="Trend",color='red')
plt.legend(loc='upper left')
plt.subplot(413)
plt.plot(seasonal,label="seasonal",color='red')
plt.legend(loc='upper left')
plt.subplot(414)
plt.plot(residual,label="Residual",color='red')
plt.legend(loc='upper left')
plt.show()

!pip install pmdarima
from pmdarima.arima import auto_arima

arima_model=auto_arima(df_resampled["Close"], start_p=1, d=1, q=1,
                       max_p=5, max_d=5, max_q=5, m=12,
                       start_P=0, D=1, start_Q=0, max_P=5, max_D=5, max_Q=5,
                       seasonal=True,
                       trace=True,
                       error_action='ignore',
                       supress_warning=True,
                       stepwise=True, n_fits=50)
#After executing this code, the arima_model variable will contain the best-fitting ARIMA model selected by the auto_arima function.
#This model can then be used for forecasting or further analysis of the time series data.

arima_model.summary()
#It displays a summary of the fitted ARIMA model, including details like the chosen model order, coefficients, statistical significance of the parameters, and goodness-of-fit metrics.

size = int(len(df_resampled) * 0.66)
x_train, x_test = df_resampled[:size], df_resampled[size:]
#this code divides the dataset into two subsets: one for training the ARIMA model and another for testing its performance on unseen data.

x_train.shape, x_test.shape

from statsmodels.tsa.statespace.sarimax import SARIMAX

model=SARIMAX(x_train["Close"],
             order=(0,1,1),
             seasonal_order=(2,1,1,12))
result=model.fit()
result.summary()

start_index=0
end_index=len(x_train)-1
train_prediction=result.predict(start_index, end_index)
train_prediction
#It generates predictions for the training data (x_train["Close"]) using the fitted SARIMAX model and stores them in train_prediction.

st_index=len(x_train)
ed_index=len(df)-1
predction=result.predict(st_index,ed_index)
predction
#this code uses the trained SARIMAX model to forecast values for the testing dataset and then shows the predicted values.

predction.plot(legend=True)
x_test['Close'].plot(legend=True)
#This code plots both the predicted and actual values on the same graph.

import math
from sklearn.metrics import mean_squared_error

# Print shapes to debug
print(f"Train Close actual: {x_train['Close'].shape}")
print(f"Train predictions: {train_prediction.shape}")
print(f"Test Close actual: {x_test['Close'].shape}")
print(f"Test predictions: {predction.shape}")

trainScore=math.sqrt(mean_squared_error(x_train['Close'],train_prediction))
testScore=math.sqrt(mean_squared_error(x_test["Close"],predction))
trainScore,testScore

